{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code_together.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d9e45ffe5b55436c93aa351320d9b2c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe3b9c26e71b4a25aa58960a8939379d",
              "IPY_MODEL_ca90c16d57484652a6cedb25ee5b8665",
              "IPY_MODEL_9829c6c3a1b84847baf9a70b56070531"
            ],
            "layout": "IPY_MODEL_342b8518cce64891b3d80dac1166e723"
          }
        },
        "fe3b9c26e71b4a25aa58960a8939379d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca95d07bb54f4f8196098f763e2909b1",
            "placeholder": "​",
            "style": "IPY_MODEL_9fcc287212084788be66dfb2b8c38c28",
            "value": "100%"
          }
        },
        "ca90c16d57484652a6cedb25ee5b8665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e35b7cfb795341b4a04794ebf9a502e6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a14d952094a4e40ae57c5ac25e4f4b5",
            "value": 2
          }
        },
        "9829c6c3a1b84847baf9a70b56070531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e245ec9fa3a94145aec921cd08b81d33",
            "placeholder": "​",
            "style": "IPY_MODEL_773877ff9b8f4d238f90fb44aded1d97",
            "value": " 2/2 [00:00&lt;00:00, 27.87it/s]"
          }
        },
        "342b8518cce64891b3d80dac1166e723": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca95d07bb54f4f8196098f763e2909b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fcc287212084788be66dfb2b8c38c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e35b7cfb795341b4a04794ebf9a502e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a14d952094a4e40ae57c5ac25e4f4b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e245ec9fa3a94145aec921cd08b81d33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "773877ff9b8f4d238f90fb44aded1d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carla-garcia-medina/nlp_final_project/blob/main/final_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Processing"
      ],
      "metadata": {
        "id": "WOmWHtFr_1wT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset cleaning & pre-process"
      ],
      "metadata": {
        "id": "R9PbnKCgwgKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk.corpus\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Please delete this if you are not using google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Please change this to your working directory\n",
        "path=\"/content/drive/My Drive/2022NLP/project/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOcN6vQdtsUS",
        "outputId": "9b3109e2-75cb-4f26-9a47-bbf4050605e8"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "closed_class_stop_words = ['a','the','an','and','or','but','about','above','after','along','amid','among',\\\n",
        "                           'as','at','by','for','from','in','into','like','minus','near','of','off','on',\\\n",
        "                           'onto','out','over','past','per','plus','since','till','to','under','until','up',\\\n",
        "                           'via','vs','with','that','can','cannot','could','may','might','must',\\\n",
        "                           'need','ought','shall','should','will','would','have','had','has','having','be',\\\n",
        "                           'is','am','are','was','were','being','been','get','gets','got','gotten',\\\n",
        "                           'getting','seem','seeming','seems','seemed',\\\n",
        "                           'enough', 'both', 'all', 'your' 'those', 'this', 'these', \\\n",
        "                           'their', 'the', 'that', 'some', 'our', 'no', 'neither', 'my',\\\n",
        "                           'its', 'his' 'her', 'every', 'either', 'each', 'any', 'another',\\\n",
        "                           'an', 'a', 'just', 'mere', 'such', 'merely' 'right', 'no', 'not',\\\n",
        "                           'only', 'sheer', 'even', 'especially', 'namely', 'as', 'more',\\\n",
        "                           'most', 'less' 'least', 'so', 'enough', 'too', 'pretty', 'quite',\\\n",
        "                           'rather', 'somewhat', 'sufficiently' 'same', 'different', 'such',\\\n",
        "                           'when', 'why', 'where', 'how', 'what', 'who', 'whom', 'which',\\\n",
        "                           'whether', 'why', 'whose', 'if', 'anybody', 'anyone', 'anyplace', \\\n",
        "                           'anything', 'anytime' 'anywhere', 'everybody', 'everyday',\\\n",
        "                           'everyone', 'everyplace', 'everything' 'everywhere', 'whatever',\\\n",
        "                           'whenever', 'whereever', 'whichever', 'whoever', 'whomever' 'he',\\\n",
        "                           'him', 'his', 'her', 'she', 'it', 'they', 'them', 'its', 'their','theirs',\\\n",
        "                           'you','your','yours','me','my','mine','I','we','us','much','and/or'\n",
        "                           ]"
      ],
      "metadata": {
        "id": "aCVb2JkktV3A"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start pre-processing\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def process_text(text):\n",
        "  \n",
        "    # Remove punctuation\n",
        "    import string\n",
        "    for punctuation in string.punctuation:\n",
        "      if punctuation!=\".\":\n",
        "        clean_text=text.replace(punctuation,' ')\n",
        "    \n",
        "    # tokenizing\n",
        "    tokens = word_tokenize(clean_text)\n",
        "\n",
        "    clean_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.isalpha():\n",
        "            clean_tokens.append(token)\n",
        "   \n",
        "    return clean_tokens\n",
        "\n",
        "#Lemminization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemma(term):\n",
        "  lemma_list=[]\n",
        "  for token in term:\n",
        "    lemma=wordnet_lemmatizer.lemmatize(token)\n",
        "    if lemma not in closed_class_stop_words:\n",
        "      lemma_list.append(lemma)\n",
        "  return lemma_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6aOZ9lytXjJ",
        "outputId": "f2f155b2-744b-42c4-ca07-a176f8ea0b31"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 30000\n",
        "val_percentage = 0.2\n",
        "test_size = 0.2 * train_size"
      ],
      "metadata": {
        "id": "zxvykSxBoMlN"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset 1: Yelp Review Rating Labeling Dataset"
      ],
      "metadata": {
        "id": "6LNriNWVuP7a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ5eJXqasJCd",
        "outputId": "49351dff-34fb-42e2-bfa0-99b0e7a34d48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.3.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_rating_labled_dataset = load_dataset(\"yelp_review_full\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "d9e45ffe5b55436c93aa351320d9b2c2",
            "fe3b9c26e71b4a25aa58960a8939379d",
            "ca90c16d57484652a6cedb25ee5b8665",
            "9829c6c3a1b84847baf9a70b56070531",
            "342b8518cce64891b3d80dac1166e723",
            "ca95d07bb54f4f8196098f763e2909b1",
            "9fcc287212084788be66dfb2b8c38c28",
            "e35b7cfb795341b4a04794ebf9a502e6",
            "2a14d952094a4e40ae57c5ac25e4f4b5",
            "e245ec9fa3a94145aec921cd08b81d33",
            "773877ff9b8f4d238f90fb44aded1d97"
          ]
        },
        "id": "5B1wfy-3uLgX",
        "outputId": "b7c1e4a9-7ba6-4e98-dd84-6f315a8b056d"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/13c31a618ba62568ec8572a222a283dfc29a6517776a3ac5945fb508877dde43)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9e45ffe5b55436c93aa351320d9b2c2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_rating_train = pd.DataFrame(yelp_rating_labled_dataset['train'])\n",
        "yelp_rating_test = pd.DataFrame(yelp_rating_labled_dataset['test'])"
      ],
      "metadata": {
        "id": "SorNWrGpuppP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_rating_train_sample = yelp_rating_train.sample(train_size)\n",
        "yelp_rating_train_sample, yelp_rating_val_sample = train_test_split(yelp_rating_train_sample, test_size = 0.1)\n",
        "yelp_rating_test_sample = yelp_rating_test.sample(test_size)"
      ],
      "metadata": {
        "id": "ink6dOplyZAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#yelp_rating_train_sample['cleaned_tokens'] = yelp_rating_train_sample['text'].apply(process_text).apply(lemma)\n",
        "#yelp_rating_test_sample['cleaned_tokens'] = yelp_rating_test_sample['text'].apply(process_text).apply(lemma)"
      ],
      "metadata": {
        "id": "jrDe1VwIvV-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_rating_train_sample.head()"
      ],
      "metadata": {
        "id": "Z2TlWC8-wXiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_rating_test_sample.head()"
      ],
      "metadata": {
        "id": "_Vf2w3HnwaTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset 2: Yelp Reviews Polarity-labled Dataset"
      ],
      "metadata": {
        "id": "SjOZr4GIwsoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data file is 4GB for this one and is too large. Therefore I decide to convert the rating-labled data above to polarized data by mapping rating score of 1 and 2 to negative, 4 and 5 to positive."
      ],
      "metadata": {
        "id": "YqUTVyyolVA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_rating_train_polarized = yelp_rating_train.sample(train_size).rename(columns={'label':'score'})\n",
        "yelp_rating_test_polarized = yelp_rating_test.sample(test_size).rename(columns={'label':'score'})\n",
        "# negative:0, neutral:1, positive:2\n",
        "polarization = {0: 0,\n",
        "         1: 0,\n",
        "         2: 1,\n",
        "         3: 2,\n",
        "         4: 2}\n",
        "yelp_rating_train_polarized[\"label\"] = yelp_rating_train_polarized[\"score\"].map(polarization)\n",
        "yelp_rating_test_polarized[\"label\"] = yelp_rating_test_polarized[\"score\"].map(polarization)"
      ],
      "metadata": {
        "id": "ejHN35dellBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#yelp_rating_train_polarized['cleaned_tokens'] = yelp_rating_train_polarized['text'].apply(process_text).apply(lemma)\n",
        "yelp_rating_train_polarized, yelp_rating_val_polarized = train_test_split(yelp_rating_train_polarized, test_size = val_percentage)\n",
        "#yelp_rating_test_polarized['cleaned_tokens'] = yelp_rating_test_polarized['text'].apply(process_text).apply(lemma)"
      ],
      "metadata": {
        "id": "6QZTR4D9mG3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_rating_train_polarized.head()\n",
        "#yelp_rating_test_polarized.groupby(\"label\").size()"
      ],
      "metadata": {
        "id": "pEs-wGfSnyKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset 3: Rotten Tomatoes Review Polarity Labeling"
      ],
      "metadata": {
        "id": "Ata25d2O0dvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rotten_tomatoes_dataset = load_dataset(\"rotten_tomatoes\")"
      ],
      "metadata": {
        "id": "Ks_oRTnR1DqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotten_tomatoes_dataset_train = pd.DataFrame(rotten_tomatoes_dataset['train'])#.sample(train_size)\n",
        "rotten_tomatoes_dataset_test = pd.DataFrame(rotten_tomatoes_dataset['test'])#.sample(test_size)"
      ],
      "metadata": {
        "id": "Dhzlv9HV1FJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rotten_tomatoes_dataset_train['cleaned_tokens'] = rotten_tomatoes_dataset_train['text'].apply(process_text).apply(lemma)\n",
        "rotten_tomatoes_dataset_train, rotten_tomatoes_dataset_val = train_test_split(rotten_tomatoes_dataset_train, test_size = val_percentage)\n",
        "#rotten_tomatoes_dataset_test['cleaned_tokens'] = rotten_tomatoes_dataset_test['text'].apply(process_text).apply(lemma)"
      ],
      "metadata": {
        "id": "AjuyRaw41z6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotten_tomatoes_dataset_train.head()\n",
        "rotten_tomatoes_dataset_test.head()"
      ],
      "metadata": {
        "id": "njd1qT8s2BUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset 4: Tweet Emoji Labeling"
      ],
      "metadata": {
        "id": "HjvKXIcP2ViF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_emoji_dataset = load_dataset(\"tweet_eval\", \"emoji\")"
      ],
      "metadata": {
        "id": "3fG9gMJv2HTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_emoji_dataset_train = pd.DataFrame(tweet_emoji_dataset['train']).sample(train_size)\n",
        "tweet_emoji_dataset_test = pd.DataFrame(tweet_emoji_dataset['test']).sample(test_size)"
      ],
      "metadata": {
        "id": "A0xV9gXdikgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tweet_emoji_dataset_train['cleaned_tokens'] = tweet_emoji_dataset_train['text'].apply(process_text).apply(lemma)\n",
        "tweet_emoji_dataset_train, tweet_emoji_dataset_val = train_test_split(tweet_emoji_dataset_train, test_size = val_percentage)\n",
        "#tweet_emoji_dataset_test['cleaned_tokens'] = tweet_emoji_dataset_test['text'].apply(process_text).apply(lemma)"
      ],
      "metadata": {
        "id": "4WbFmO0Zi2fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_emoji_dataset_train.head()\n",
        "tweet_emoji_dataset_test.head()\n",
        "#len(sorted(tweet_emoji_dataset_train['label'].unique()))"
      ],
      "metadata": {
        "id": "JaJs5Fs3kXcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List all datasets"
      ],
      "metadata": {
        "id": "3uYp0Qebg8Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_lst = [(yelp_rating_train_sample, yelp_rating_val_sample, yelp_rating_test_sample, 5),\n",
        "            (yelp_rating_train_polarized, yelp_rating_val_polarized, yelp_rating_test_polarized, 3),\n",
        "            (rotten_tomatoes_dataset_train, rotten_tomatoes_dataset_val, rotten_tomatoes_dataset_test, 2),\n",
        "            (tweet_emoji_dataset_train, tweet_emoji_dataset_val, tweet_emoji_dataset_test, 20)]"
      ],
      "metadata": {
        "id": "8uDx-lzhg66f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building BiLSTM Model"
      ],
      "metadata": {
        "id": "0lYtWo7S_64K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_VZkGbgO4yA"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpbnKsQeptXw"
      },
      "source": [
        "!pip install sacremoses\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import sacremoses\n",
        "from torch.utils.data import dataloader, Dataset\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Sm_iuR_hJp2"
      },
      "source": [
        "## Download and Load GloVe Embeddings\n",
        "We will use GloVe embedding parameters to initialize our layer of word representations / embedding layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRCcCtcSjEPR"
      },
      "source": [
        "# === Download GloVe word embeddings\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "# === Unzip word embeddings and use only the top 50000 word embeddings for speed\n",
        "# !unzip glove.6B.zip\n",
        "# !head -n 50000 glove.6B.300d.txt > glove.6B.300d__50k.txt\n",
        "\n",
        "# === Download Preprocessed version\n",
        "!wget https://docs.google.com/uc?id=1KMJTagaVD9hFHXFTPtNk0u2JjvNlyCAu -O glove_split.aa\n",
        "!wget https://docs.google.com/uc?id=1LF2yD2jToXriyD-lsYA5hj03f7J3ZKaY -O glove_split.ab\n",
        "!wget https://docs.google.com/uc?id=1N1xnxkRyM5Gar7sv4d41alyTL92Iip3f -O glove_split.ac\n",
        "!cat glove_split.?? > 'glove.6B.300d__50k.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSF0C4jHjnSz"
      },
      "source": [
        "def load_glove(glove_path, embedding_dim):\n",
        "    with open(glove_path) as f:\n",
        "        token_ls = [PAD_TOKEN, UNK_TOKEN]\n",
        "        embedding_ls = [np.zeros(embedding_dim), np.random.rand(embedding_dim)]\n",
        "        for line in f:\n",
        "            token, raw_embedding = line.split(maxsplit=1)\n",
        "            token_ls.append(token)\n",
        "            embedding = np.array([float(x) for x in raw_embedding.split()])\n",
        "            embedding_ls.append(embedding)\n",
        "        embeddings = np.array(embedding_ls)\n",
        "        print(embedding_ls[-1].size)\n",
        "    return token_ls, embeddings\n",
        "\n",
        "PAD_TOKEN = '<PAD>'\n",
        "UNK_TOKEN = '<UNK>'\n",
        "EMBEDDING_DIM=300 # dimension of Glove embeddings\n",
        "glove_path = \"glove.6B.300d__50k.txt\"\n",
        "vocab, embeddings = load_glove(glove_path, EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert text data into sequence of indices"
      ],
      "metadata": {
        "id": "r8sa03PTIc6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(data, labels, tokenizer, vocab, max_seq_length=128):\n",
        "    vocab_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "    text_data = []\n",
        "    label_data = []\n",
        "    for ex in tqdm(data):\n",
        "        tokenized = tokenizer.tokenize(ex.lower())\n",
        "        ids = [vocab_to_idx.get(token, 1) for token in tokenized]\n",
        "        text_data.append(ids)\n",
        "    return text_data, labels"
      ],
      "metadata": {
        "id": "3TMeeDL3Ib4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dlUYNvgPUXs"
      },
      "source": [
        "## Create DataLoaders\n",
        "Create Pytorch DataLoaders for our train, val, and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uJDfnVMxBsz"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
        "    This class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, data_list, target_list, max_sent_length=128):\n",
        "        \"\"\"\n",
        "        @param data_list: list of data tokens \n",
        "        @param target_list: list of data targets \n",
        "        \"\"\"\n",
        "        self.data_list = data_list\n",
        "        self.target_list = target_list\n",
        "        self.max_sent_length = max_sent_length\n",
        "        assert (len(self.data_list) == len(self.target_list))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "        \n",
        "    def __getitem__(self, key, max_sent_length=None):\n",
        "        \"\"\"\n",
        "        Triggered when calling dataset[i]\n",
        "        \"\"\"\n",
        "        if max_sent_length is None:\n",
        "            max_sent_length = self.max_sent_length\n",
        "        token_idx = self.data_list[key][:max_sent_length]\n",
        "        label = self.target_list[key]\n",
        "        return [token_idx, label]\n",
        "\n",
        "    def collate_func(self, batch):\n",
        "        \"\"\"\n",
        "        Customized function for DataLoader that dynamically pads the batch so that all \n",
        "        data have the same length\n",
        "        \"\"\" \n",
        "        data_list = [] # store padded sequences\n",
        "        label_list = [element[1] for element in batch]\n",
        "        max_batch_seq_len = None # the length of longest sequence in batch\n",
        "                                 # if it is less than self.max_sent_length\n",
        "                                 # else max_batch_seq_len = self.max_sent_length\n",
        "\n",
        "        # If self.max_sent_length is less than the length of longest sequence \n",
        "        # in the batch, use self.max_sent_length. Otherwise, use the length \n",
        "        # of longest sequence in the batch.\n",
        "        max_num_elements = max([len(element[0]) for element in batch])\n",
        "        if max_num_elements < self.max_sent_length:\n",
        "          max_batch_seq_len = max_num_elements\n",
        "        else:\n",
        "          max_batch_seq_len = self.max_sent_length\n",
        "\n",
        "        \"\"\"\n",
        "          # Pad the sequences in your data \n",
        "          # Trim the sequences that are longer than self.max_sent_length\n",
        "          # return padded data_list and label_list\n",
        "        \"\"\"\n",
        "\n",
        "        for element in batch:\n",
        "          sequence = element[0]\n",
        "          length = len(sequence)\n",
        "          if length < max_batch_seq_len:\n",
        "            padding = [0 for _ in range(max_batch_seq_len - length)]\n",
        "            data_list.append(sequence + padding)\n",
        "          else:\n",
        "            data_list.append(sequence[:max_batch_seq_len])\n",
        "        \n",
        "        data_list = torch.tensor(data_list)\n",
        "        label_list = torch.tensor(label_list)\n",
        "\n",
        "        return [data_list, label_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpdnYbPIgNXw"
      },
      "source": [
        "## BiLSTM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOYEmADNDVIh"
      },
      "source": [
        "# First import torch related libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTMClassifier classification model\n",
        "    \"\"\"\n",
        "    def __init__(self, embeddings, hidden_size, num_layers, num_classes, bidirectional, dropout_prob=0.3):\n",
        "        \"\"\"\n",
        "           Components of BiLSTM Classifier model\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embedding_layer = self.load_pretrained_embeddings(embeddings)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embeddings.shape[1], hidden_size=hidden_size, \n",
        "            num_layers=num_layers, dropout=dropout_prob, \n",
        "            batch_first=True, bidirectional=bidirectional)\n",
        "        self.non_linearity = nn.ReLU() # For example, ReLU\n",
        "        self.clf = nn.Linear(hidden_size*2, num_classes) # classifier layer\n",
        "        \n",
        "    \n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        embedding_layer = nn.Embedding(embeddings.shape[0], embeddings.shape[1], padding_idx=0)\n",
        "        embedding_layer.weight.data = torch.Tensor(embeddings).float()\n",
        "        return embedding_layer\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        logits = None\n",
        "        v_embedded = self.embedding_layer(inputs)\n",
        "        v_dropout = self.dropout(v_embedded)\n",
        "        v_bilstm, _ = self.lstm(v_dropout)\n",
        "        v_avg_pool = torch.mean(v_bilstm, 1)\n",
        "        v_nonlinear = self.non_linearity(v_avg_pool)\n",
        "        v_classify = self.clf(v_nonlinear)\n",
        "\n",
        "        return v_classify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwkHYzbQikT4"
      },
      "source": [
        "## Train model with early stopping\n",
        "\n",
        "Train the model for `NUM_EPOCHS`. \n",
        "Keep track of training loss.  \n",
        "Compute the validation accuracy after each epoch. Keep track of the best validation accuracy and save the model with the best validation accuracy.  \n",
        "\n",
        "If the validation accuracy does not improve for more than `early_stop_patience` number of epochs in a row, stop training. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, device):\n",
        "    accuracy = None\n",
        "    n_correct = n_total = 0 \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (data_batch, batch_labels) in dataloader:\n",
        "            out = model(data_batch.to(device))\n",
        "            max_scores, preds = out.max(dim=1)\n",
        "            n_correct += np.sum(preds.cpu().numpy() == batch_labels.numpy())\n",
        "            n_total += out.shape[0]\n",
        "    accuracy = n_correct*1.0/n_total\n",
        "    return accuracy "
      ],
      "metadata": {
        "id": "s5_H1tmQ-sVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOlop_TMOD9V"
      },
      "source": [
        "def train_with_early_stopping(device, criterion, optimizer):\n",
        "  train_loss_history = []\n",
        "  val_accuracy_history = []\n",
        "  best_val_accuracy = 0\n",
        "  n_no_improve = 0\n",
        "  early_stop_patience=2\n",
        "  NUM_EPOCHS=10\n",
        "    \n",
        "  for epoch in tqdm(range(NUM_EPOCHS)):\n",
        "      model.train()  # this enables dropout/regularization\n",
        "      for i, (data_batch, batch_labels) in enumerate(train_loader):\n",
        "          preds = model(data_batch.to(device))\n",
        "          loss = criterion(preds, batch_labels.to(device))\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "          train_loss_history.append(loss.item())\n",
        "          \n",
        "      \"\"\"\n",
        "          Code for tracking best validation accuracy, saving the best model, and early stopping\n",
        "          # Compute validation accuracy after each training epoch using `evaluate` function\n",
        "          # Keep track of validation accuracy in `val_accuracy_history`\n",
        "          # save model with best validation accuracy, hint: torch.save(model, 'best_model.pt')\n",
        "          # Early stopping: \n",
        "          # stop training if the validation accuracy does not improve for more than `early_stop_patience` runs\n",
        "      \"\"\"\n",
        "      accuracy = evaluate(model, val_loader, device)\n",
        "      val_accuracy_history.append(accuracy)\n",
        "      torch.save(model, 'best_model.pt')\n",
        "      if best_val_accuracy < accuracy:\n",
        "        best_val_accuracy = accuracy\n",
        "      else:\n",
        "        n_no_improve += 1\n",
        "      if n_no_improve == early_stop_patience:\n",
        "        break\n",
        "\n",
        "  print(\"Best validation accuracy is: \", best_val_accuracy)\n",
        "  return train_loss_history, val_accuracy_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hhFpkMHnT7Z"
      },
      "source": [
        "To avoid overfiting of our model, we use early stopping. Particularly when training a large model, early stopping can help us stop training when at the point where the model stops making genaralizations about the data and begins learning statistical noise that would cause the model to overfit.This would make our model less useful and have less performance when tested on new data/datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Evaluation"
      ],
      "metadata": {
        "id": "tehvzfDXjhm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score(keyFileName, responseFileName):\n",
        "    keyFile = open(keyFileName, 'r')\n",
        "    key = keyFile.readlines()\n",
        "    responseFile = open(responseFileName, 'r')\n",
        "    response = responseFile.readlines()\n",
        "    if len(key) != len(response):\n",
        "        print(\"length mismatch between key and submitted file\")\n",
        "        exit()\n",
        "    correct = 0\n",
        "    incorrect = 0\n",
        "    response_total = 0\n",
        "    key_total = 0\n",
        "\n",
        "    for i in range(len(key)):\n",
        "        key[i] = key[i].rstrip('\\n')\n",
        "        response[i] = response[i].rstrip('\\n')\n",
        "\n",
        "        if key[i] == response[i]:\n",
        "            correct += 1\n",
        "        else:\n",
        "            incorrect += 1\n",
        "\n",
        "        if response[i]:\n",
        "            response_total += 1\n",
        "        if key[i]:\n",
        "            key_total += 1\n",
        "\n",
        "    print(correct, \"out of\", str(correct + incorrect) + \" tags correct\")\n",
        "    accuracy = 100.0 * correct / (correct + incorrect)\n",
        "    print(\"  accuracy: %5.2f\" % accuracy)\n",
        "\n",
        "    precision = 100.0 * correct / response_total\n",
        "    recall = 100.0 * correct / key_total\n",
        "    F = 2 * precision * recall / (precision + recall)\n",
        "    print(\"  precision: %5.2f\" % precision)\n",
        "    print(\"  recall:    %5.2f\" % recall)\n",
        "    print(\"  F1:        %5.2f\" % F)"
      ],
      "metadata": {
        "id": "GrgNfXxjj0v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Put it all together"
      ],
      "metadata": {
        "id": "GvMYLl59hucO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for datasets in datasets_lst:\n",
        "  train_labels = datasets[0]['label'].tolist()\n",
        "  train_texts = datasets[0]['text'].tolist()\n",
        "  val_labels = datasets[1]['label'].tolist()\n",
        "  val_texts = datasets[1]['text'].tolist()\n",
        "  test_labels = datasets[2]['label'].tolist()\n",
        "  test_texts = datasets[2]['text'].tolist()\n",
        "  num_classes = datasets[3]\n",
        "\n",
        "  tokenizer = sacremoses.MosesTokenizer()\n",
        "  train_data_indices, train_labels = tokenize(train_texts, train_labels, tokenizer, vocab)\n",
        "  val_data_indices, val_labels = tokenize(val_texts, val_labels, tokenizer, vocab)\n",
        "  test_data_indices, test_labels = tokenize(test_texts, test_labels, tokenizer, vocab)\n",
        "\n",
        "  BATCH_SIZE = 64\n",
        "  max_sent_length=128\n",
        "\n",
        "  train_dataset = SpamDataset(train_data_indices, train_labels, max_sent_length)\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            collate_fn=train_dataset.collate_func,\n",
        "                                            shuffle=True)\n",
        "\n",
        "  val_dataset = SpamDataset(val_data_indices, val_labels, train_dataset.max_sent_length)\n",
        "  val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            collate_fn=train_dataset.collate_func,\n",
        "                                            shuffle=False)\n",
        "\n",
        "  test_dataset = SpamDataset(test_data_indices, test_labels, train_dataset.max_sent_length)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            collate_fn=train_dataset.collate_func,\n",
        "                                            shuffle=False)\n",
        "\n",
        "  data_batch, labels = next(iter(train_loader))\n",
        "  # BiLSTM hyperparameters\n",
        "  hidden_size = 32\n",
        "  num_layers = 1\n",
        "  num_classes = num_classes\n",
        "  print(num_classes)\n",
        "  bidirectional=True\n",
        "  torch.manual_seed(1234)\n",
        "\n",
        "  # if cuda exists, use cuda, else run on cpu\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda:0\")\n",
        "  else:\n",
        "      device=torch.device('cpu')\n",
        "\n",
        "  model = LSTMClassifier(embeddings, hidden_size, num_layers, num_classes, bidirectional)\n",
        "  model.to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "  train_loss_history, val_accuracy_history = train_with_early_stopping(device, criterion, optimizer)\n",
        "\n",
        "  #train_loss_history\n",
        "  pd.Series(train_loss_history).plot()\n",
        "  plt.show()\n",
        "\n",
        "  #X-axis: Epochs, Y-axis: validation accuracy\n",
        "  pd.Series(val_accuracy_history).plot()\n",
        "  plt.show()\n",
        "\n",
        "  # Reload best model from saved checkpoint\n",
        "  # Compute test accuracy\n",
        "  # device = \"cuda:0\"\n",
        "  model = torch.load('best_model.pt')\n",
        "  test_accuracy = evaluate(model, test_loader, device)\n",
        "  print(test_accuracy)\n",
        "\n",
        "  #score(key_file, response_file)"
      ],
      "metadata": {
        "id": "RWl07HQQb1Z2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}